{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMos+vptnQGQwZyIXDW2QWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Behnaz81/MachineLearningDaily/blob/main/calculus_for_machine_learning/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient\n",
        "\n",
        "Gradient of $f(x, y)$ is a vector of partial derivatives with respect to each variables.\n",
        "\n",
        "$$\n",
        "\\nabla f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "$$\n",
        "f(x) = x^2 + y^2\n",
        "\\\\\n",
        "\\nabla f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ 2y \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "To find maximums and minimums in functions with more than one variable, we should set the partial derivatives to 0 and solve the equations system.\n",
        "\n",
        "But solving these equations can be very expensive. Another way to solve them is using **gradient descent**."
      ],
      "metadata": {
        "id": "e1CW_5SaWcqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent\n",
        "\n",
        "It's a method to find minimum of a function. As mentioned before finding where the derivative is 0 can be hard, gradient descent helps us do it faster and easier.\n",
        "\n",
        "In gradient descent we choose a random point and according to the slope of function in that point we move till we find the minimum.\n",
        "\n",
        "$$\n",
        "(x_k, y_k) = (x_{k-1}, y_{k-1}) - \\alpha f'(x)\n",
        "$$\n",
        "\n",
        "$\\alpha$ is called the **learning rate** it helps to set the step of gradient descent.\n",
        "\n",
        "### Drawbacks of gradient descent\n",
        "\n",
        "1. **Learning rate should be the right number.** If it's too big it may miss the minimum point and if it's too small it may take too long to get to the minimum. Also finding the right amount doesn't have any rules.\n",
        "\n",
        "2. **Getting stuck in local minimum.** The functions may have many local minimums and if we only do gradient descent once it may not find the global minimum. To solve this issue you should do gradient descent more than once with more than one random start point to have a higher chance to find the global minimum.\n",
        "\n",
        "You can use gradient descent to find $(m,b)$ with minimum loss in linear regression. If we have $n$ points $\\{(x_1, y_1), (x_2, y_2), ... (x_n, y_n)\\}$ the total loss function is:\n",
        "\n",
        "$$\n",
        "l(m,b) = \\frac{1}{2n}[(mx_1+b-y_1)^2+...+(mx_n+b-y_n)^2]\n",
        "$$\n",
        "\n",
        "To find the optimum $m$ and $b$ we use gradient descent.\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}m_N \\\\ b_N\\end{pmatrix} = \\begin{pmatrix}m_{N-1}\\\\ b_{N-1}\\end{pmatrix} - \\alpha \\nabla l_1(m_{N-1}, b_{N-1})\n",
        "$$"
      ],
      "metadata": {
        "id": "9gWTSGWR3yTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m64Q6JMkWZCE"
      },
      "outputs": [],
      "source": []
    }
  ]
}